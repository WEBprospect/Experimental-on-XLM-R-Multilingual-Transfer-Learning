# Cross-Lingual Transfer Experiments: Zero-Shot to Full Fine-Tuning
## OVERVIEW

이 프로젝트는 **XLM-RoBERTa(XLM-R)** 모델을 기반으로,  
**다국어 개체명 인식(NER)** 태스크에서 언어 간 전이학습(Cross-Lingual Transfer Learning)의 효과를 체계적으로 검증한 실험 연구입니다.  

특히, **저자원 언어 환경**에서 모델이 얼마나 효율적으로 일반화할 수 있는지를 관찰하기 위해  
다음 네 가지 학습 전략을 동일한 파이프라인에서 비교했습니다.

- **Zero-shot Transfer**: 독일어 데이터로만 학습한 모델을 프랑스어, 이탈리아어, 영어 등 다른 언어에 직접 적용  
- **Few-shot Transfer**: 소량의 타겟 언어(프랑스어) 데이터를 점진적으로 추가해 성능 향상 곡선 관찰  
- **Multilingual Fine-tuning**: 두 개 이상의 언어(예: 독일어 + 프랑스어)를 동시에 학습시켜 교차 일반화 효과 분석  
- **Full Fine-tuning**: 모든 언어 데이터를 통합해 모델을 완전 미세조정, 최대 성능 한계 측정  

실험에는 **PAN-X(WikiAnn)** 데이터셋의 **독일어(de)**, **프랑스어(fr)**, **이탈리아어(it)**, **영어(en)** 데이터를 사용했으며,  
언어별 데이터 비율을 의도적으로 불균형하게 조정하여 실제 글로벌 환경에서의 모델 적응 시나리오를 모사했습니다.  

모든 실험은 동일한 모델 구조, 동일한 하이퍼파라미터 설정하에서 수행되어  
**데이터 크기, 언어 조합, 학습 전략에 따른 전이 성능 차이**를 정량적으로 비교할 수 있도록 설계되었습니다.  

이 프로젝트를 통해 다음과 같은 주요 질문을 검증했습니다:
1. 한 언어로 학습된 XLM-R 모델이 다른 언어에 어느 정도 일반화되는가?  
2. 타겟 언어 데이터가 얼마나 있어야 Zero-shot을 능가하는가?  
3. 다국어 동시 학습이 개별 언어 성능에 어떤 영향을 미치는가?  


## 실험 설계
| 항목 | 설정 |
|---|---|
| **모델** | xlm-roberta-base |
| **태스크** | Named Entity Recognition (BIO 스키마: B/I-PER, B/I-ORG, B/I-LOC, O) |
| **데이터셋** | PAN-X (WikiAnn) |
| **언어** | 독일어(de), 프랑스어(fr), 이탈리아어(it), 영어(en) |
| **평가지표** | F1-score (seqeval 라이브러리) |

의도적으로 데이터 불균형을 구성하여 실제 시나리오를 모사했습니다.  
비율은 독일어 62.9%, 프랑스어 22.9%, 이탈리아어 8.4%, 영어 5.9%로 설정했습니다.

---




## TRAINING PIPELINE

모든 실험은 동일한 모델 구조와 하이퍼파라미터 설정하에서 수행되어,  
전이 전략의 차이에 따른 성능 변화만을 비교할 수 있도록 설계되었습니다.  
파이프라인은 다음 5단계로 구성됩니다.

1. **데이터 전처리 (Token-Label Alignment)**  
   XLM-RoBERTa는 서브워드 단위 토크나이저를 사용하기 때문에,  
   단어 단위 레이블을 서브워드 단위로 재정렬하는 과정이 필요합니다.  
   각 단어의 첫 번째 서브워드에는 원본 레이블을 부여하고,  
   나머지 서브워드는 손실 계산에서 제외하기 위해 `-100`으로 마스킹했습니다.  
   이를 통해 모델이 토큰 수준이 아닌 **단어 단위 NER 인식**을 학습하도록 했습니다.

2. **데이터 인코딩**  
   Hugging Face `Datasets`의 `map()` 함수를 활용하여  
   전처리 함수를 모든 데이터 split(train, validation, test)에 병렬 적용했습니다.  
   이 단계에서 `'tokens'`, `'langs'`, `'ner_tags'` 등의 원본 컬럼을 제거하고  
   모델 입력에 필요한 텐서 형식(`input_ids`, `attention_mask`, `labels`)으로 변환했습니다.

3. **모델 정의**  
   `XLM-RoBERTa Base`를 기반으로 토큰 분류용 헤드를 추가했습니다.  
   이 헤드는 Dropout 층과 Linear 분류층으로 구성되어 있으며,  
   각 토큰의 은닉 벡터(hidden state)에 대해 NER 레이블 확률을 출력합니다.  
   손실 함수는 `CrossEntropyLoss`를 사용하여 각 토큰 단위의 분류 오차를 계산했습니다.

4. **학습 구조**  
   Hugging Face의 `Trainer` API를 이용해 학습 및 검증 과정을 자동화했습니다.  
   학습 파라미터는 다음과 같습니다.  
   - 학습률: 5e-5  
   - Epoch: 3  
   - Batch size: 16  
   - Weight decay: 0.05  
   - Evaluation strategy: `"epoch"`  
   학습 중 매 epoch마다 검증 F1-score를 계산하며,  
   최적 모델 가중치는 자동으로 저장되었습니다.

5. **평가 방식**  
   모델 출력(logits)을 argmax 연산으로 변환한 뒤,  
   `seqeval` 라이브러리를 이용해 **엔티티 단위 F1-score**를 계산했습니다.  
   손실 계산에서 제외된 `-100` 토큰은 평가에서도 무시됩니다.  
   이 지표를 통해 언어별, 데이터 규모별 전이 성능을 비교했습니다.

---

## RESULTS

### Zero-Shot Transfer (독일어 모델 → 타 언어)
독일어 데이터로만 학습한 모델을 다른 언어 데이터에 직접 적용했습니다.  
언어 계열이 유사한 프랑스어에서는 비교적 높은 전이 성능을 보였고,  
언어적 거리가 먼 영어로 갈수록 성능이 점진적으로 감소했습니다.

| 평가 언어 | de | fr | it | en |
|:---|---:|---:|---:|---:|
| **F1-score** | **0.875** | 0.704 | ~0.70 | 0.606 |

→ **게르만어 ↔ 로망어 간 구조적 차이**가 성능 격차의 주요 요인으로 작용했습니다.

---

### Few-Shot Transfer (프랑스어 데이터 증강)
프랑스어의 학습 샘플 수를 점진적으로 늘려가며  
Zero-shot 대비 성능 향상 추이를 분석했습니다.  
데이터가 적을 때는 전이 효과가 빠르게 증가했지만, 일정 수준 이후에는 향상 폭이 감소했습니다.

| 프랑스어 샘플 수 | 250 | 500 | 750 | 1,000 | 2,000 | 4,000 |
|:---:|---:|---:|---:|---:|---:|---:|
| **F1-score** | ~0.65 | ~0.68 | **~0.70** | ~0.72 | ~0.76 | ~0.80 |

→ 약 **750개 샘플**에서 Zero-shot과 동등한 수준에 도달했으며,  
그 이후에는 **체감 효율이 감소하는 수익 체감 구간**에 들어갔습니다.

---

### Multilingual Fine-Tuning (독일어 + 프랑스어)
두 언어를 동시에 학습시켜 **언어 간 시너지 효과**를 검증했습니다.  
결과적으로 프랑스어 성능은 크게 향상되었고,  
독일어 성능은 유지되었으며, 이탈리아어와 영어에서도 미세한 전이 이득이 확인되었습니다.

| 평가 언어 | de | fr | it | en |
|:---|---:|---:|---:|---:|
| **F1-score** | **0.876** | **0.865** | 0.784 | 0.661 |

→ 다국어 학습을 통해 **언어 간 공통 표현 공간이 강화**되었음을 확인했습니다.

---

### Full Multilingual Fine-Tuning (de + fr + it + en)
모든 언어 데이터를 통합 학습하여 가장 균형 잡힌 성능을 달성했습니다.  
각 언어의 편향이 줄어들며 전반적으로 안정적인 F1-score를 보였고,  
특정 언어에 과도하게 최적화되는 현상이 완화되었습니다.

---

## INSIGHTS & ANALYSIS

1. **Zero-shot의 효율성**  
   - 레이블이 없는 저자원 언어에서도 중간 수준의 성능 확보  
   - 빠른 모델 배포 및 초기 적용에 유리  

2. **Few-shot의 임계점**  
   - 약 750~1,000개 수준에서 Zero-shot을 능가  
   - 데이터가 증가할수록 수익 체감 현상 발생  

3. **Multilingual 시너지 효과**  
   - 서로 다른 언어 간 표현 공유로 모든 언어의 F1-score가 상승  
   - 학습에 포함되지 않은 언어에서도 일반화 효과 확인  

4. **언어적 거리의 영향**  
   - 언어 계열이 유사할수록 전이 효과가 높음  
   - 게르만어(독일어) → 로망어(프랑스어, 이탈리아어)는 강한 전이  
   - 영어처럼 문법 구조가 상이한 언어에서는 하락폭이 큼  

5. **실무적 전략 제안**  
   - **데이터 제한 환경:** Zero-shot 또는 소량 Few-shot으로 접근  
   - **균형 성능 목표:** Multilingual Fine-tuning  
   - **도메인 특화:** 소규모 Few-shot Fine-tuning으로 조정  

---

## TECH STACK
| 구성 요소 | 세부 내용 |
|:--|:--|
| **Framework** | PyTorch, Hugging Face Transformers, Datasets |
| **Evaluation** | seqeval (F1-score for NER) |
| **Utilities** | pandas, numpy |
| **Environment** | Python 3.8+, CUDA GPU, Google Colab / Local |

---




